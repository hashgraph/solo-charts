global:
  namespaceOverride: ""

# cloud configuration
cloud:
  # if generateNewSecrets set false, the secret will not be rendered in template,
  # instead, secrets will be created from solo in run time
  generateNewSecrets: true
  buckets:
    streamBucket: "solo-streams"
    streamBucketPrefix: ""
    streamBucketRegion: "us-east-1"
    backupBucket: "solo-backups" # does not need prefix to save to subfolder, just use "bucket/folder"
  minio:
    enabled: true
  s3:
    enabled: false
  gcs:
    enabled: false

cheetah:
  image:
    registry: "ghcr.io"
    repository: "hashgraph/solo-cheetah/cheetah"
    tag: "0.2.0-20250522"
    pullPolicy: "IfNotPresent"
  # whether it should stop the pipelines if any error occurs; if false, it will continue with the expectation that the
  # error may get fixed in the next run
  stopOnError: false
  deployment:
    # if true, a single instance of cheetah will be deployed for all stream files (i.e. record-streams, events-streams, block-streams etc.)
    # WARNING: Other steams files uploader sidecar configuration will be ignored
    single: false
    nameOverride: "cheetah-stream-uploader"
    resources: {}

# telemetry configurations
telemetry:
  prometheus:
    svcMonitor:
      enabled: true

# reduce default termination grace period
terminationGracePeriodSeconds: 10

# helm test container
tester:
  deployPodMonitor: true
  clusterRoleName: "pod-monitor-role" # this is a shared cluster role for all namespaces
  image:
    registry: "ghcr.io"
    repository: "hashgraph/solo-containers/kubectl-bats"
    tag: "0.38.0"
    pullPolicy: "IfNotPresent"
  resources: {}

# default settings for a single node
# This default configurations can be overridden for each node in the hedera.nodes section.
defaults:
  volumeClaims:
    enabled: false
    storageClassName: ""
    node:
      eventStreams: "100Gi"
      recordStreams: "100Gi"
      blockStreams: "100Gi"
      recordStreamsSidecar: "100Gi"
      dataOnboard: "1Gi"
      dataSaved: "500Gi"
      dataStats: "50Gi"
      dataUpgrade: "5Gi"
      output: "5Gi"
      config: "1Gi"
      keys: "1Gi"
      dataLibs: "1Gi"
      dataApps: "1Gi"
  root: # root container
    image:
      registry: "ghcr.io"
      repository: "hashgraph/solo-containers/ubi8-init-java21"
      tag: "0.38.0"
      pullPolicy: "IfNotPresent"
    resources: {}
    extraEnv: []
  consensus:
    service:
      type: "NodePort"
      externalTrafficPolicy: "Local"
      annotations: { }
      labels: { }
  haproxy:
    enabled: true
    nameOverride: "haproxy"
    image:
      registry: "docker.io"
      repository: "haproxytech/haproxy-alpine"
      tag: "2.4.25"
      pullPolicy: "IfNotPresent"
    resources: {}
    service:
      type: "NodePort"
      externalTrafficPolicy: "Local"
      annotations: {}
      labels: {}
  envoyProxy:
    enabled: true
    nameOverride: "envoy-proxy"
    image:
      registry: "docker.io"
      repository: "envoyproxy/envoy"
      tag: "v1.21.1"
      pullPolicy: "IfNotPresent"
    resources: {}
    service:
      type: "NodePort"
      externalTrafficPolicy: "Local"
      annotations: {}
      labels: {}
  sidecars:
    recordStreamUploader:
      enabled: true
      nameOverride: "record-stream-uploader"
      resources: {}
    recordStreamSidecarUploader:
      enabled: true
      nameOverride: "record-stream-sidecar-uploader"
      resources: {}
    eventStreamUploader:
      enabled: true
      nameOverride: "event-stream-uploader"
      resources: {}
    blockstreamUploader:
      enabled: true
      nameOverride: "blockstream-uploader"
      resources: {}
    backupUploader:
      enabled: false
      type: "s3"
      provider: "GCS"
      nameOverride: "backup-uploader"
      image:
        registry: "ghcr.io"
        repository: "hashgraph/solo-containers/backup-uploader"
        tag: "0.38.0"
        pullPolicy: "IfNotPresent"
      config:
        backupBucket: "backup"
      resources: {}
    otelCollector:
      enabled: true
      nameOverride: ""
      image:
        registry: "docker.io"
        repository: "otel/opentelemetry-collector-contrib"
        tag: "0.72.0"
        pullPolicy: "IfNotPresent"
      resources: {}
      receivers:
        prometheus:
          scrapeTargets: [ 0.0.0.0:9999 ]  # hedera node metrics are exposed at port 9999
          scrapeInterval: 5s
      exporters:
        otlp:
          endpoint: tempo:4317
          tls:
            insecure: true
        prometheus:
          tls:
            insecure: true
        prometheusRemoteWrite:
          enabled: false
          endpoint: "" # e.g. http://prometheus.<NAMESPACE>.svc:9090/api/v1/write
          tls:
            insecure: true

# This configures the minio tenant subchart
# Reference for configuration: https://github.com/minio/operator/blob/master/helm/tenant/values.yaml
minio-server:
  tenant:
    buckets:
      - name: solo-streams
      - name: solo-backups
    name: minio
    pools:
      - servers: 1
        name: pool-1
        volumesPerServer: 1
        size: 10Gi
        nodeSelector: { }
        labels:
          solo.hedera.com/testSuiteName: ""
          solo.hedera.com/testName: ""
          solo.hedera.com/testRunUID: ""
          solo.hedera.com/testCreationTimestamp: ""
          solo.hedera.com/testExpirationTimestamp: ""
          solo.hedera.com/testRequester: ""
        tolerations:
          - key: "solo-scheduling.io/os"
            operator: "Equal"
            value: "linux"
            effect: "NoSchedule"
          - key: "solo-scheduling.io/role"
            operator: "Equal"
            value: "network"
            effect: "NoSchedule"
    configuration:
      name: minio-secrets
    certificate:
      requestAutoCert: false
  environment:
    MINIO_BROWSER_LOGIN_ANIMATION: off # https://github.com/minio/console/issues/2539#issuecomment-1619211962

# hedera-mirror-node-explorer configuration
# common deployment configuration
deployment:
  init:
    enableDnsValidation: true
  podAnnotations: {}
  podLabels: {}
  nodeSelector: {}
  tolerations:
    - key: "solo-scheduling.io/os"
      operator: "Equal"
      value: "linux"
      effect: "NoSchedule"
    - key: "solo-scheduling.io/role"
      operator: "Equal"
      value: "network"
      effect: "NoSchedule"
  # Specify pod affinity
  # Use complete affinity spec starting with key "nodeAffinity:"
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  affinity: {}
  priorityClassName: {}
  ## PodDisruptionBudget for solo testing pods
  ## Default backend Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  ## @param deployment.podDisruptionBudget.create Enable Pod Disruption Budget configuration
  ## @param deployment.podDisruptionBudget.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param deployment.podDisruptionBudget.maxUnavailable Maximum number/percentage of pods that should remain scheduled
  ##
  podDisruptionBudget:
    create: true
    minAvailable: 1
    maxUnavailable: ""
  testMetadata:
    enabled: false
    testSuiteName: ""
    testName: ""
    testRunUID: ""
    testCreationTimestamp: ""
    testExpirationTimestamp: ""
    testRequester: ""

# haproxy specific deployment configuration
haproxyDeployment:
  podAnnotations: { }
  podLabels: { }
  nodeSelector: { }
  tolerations: [ ]
  # Specify pod affinity
  # Use complete affinity spec starting with key "nodeAffinity:"
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  affinity: { }
  priorityClassName: { }

# envoy specific deployment configuration
envoyDeployment:
  podAnnotations: { }
  podLabels: { }
  nodeSelector: { }
  tolerations: [ ]
  # Specify pod affinity
  # Use complete affinity spec starting with key "nodeAffinity:"
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  affinity: { }
  priorityClassName: { }

# hedera node configuration
hedera:
  initContainers: []

  configMaps:
    apiPermissionsProperties: ""
    applicationEnv: ""
    applicationProperties: ""
    bootstrapProperties: ""
    configTxt: ""
    log4j2Xml: ""
    settingsTxt: ""
    genesisNetworkJson: ""
    genesisThrottlesJson: ""
    blockNodesJson: ""

  # Only the name of the node is required. The rest of the configuration will be inherited from `defaults` section
  nodes:
    - name: node1
      nodeId: 0
      accountId: 0.0.3
      envoyProxyStaticIP: "" # Static IP for Envoy, leave empty to not use
      haproxyStaticIP: "" # Static IP for HAProxy can be empty if not used
      nodeSelector: { }
      tolerations: [ ]
      # Specify pod affinity
      # Use complete affinity spec starting with key "nodeAffinity:"
      # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
      affinity: { }
      priorityClassName: { }

    - name: node2
      nodeId: 1
      accountId: 0.0.4
      envoyProxyStaticIP: ""
      haproxyStaticIP: ""
      nodeSelector: { }
      tolerations: [ ]
      affinity: { }
      priorityClassName: { }

    - name: node3
      nodeId: 2
      accountId: 0.0.5
      envoyProxyStaticIP: ""
      haproxyStaticIP: ""
      nodeSelector: { }
      tolerations: [ ]
      affinity: { }
      priorityClassName: { }
